{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from parameters import *\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lasso_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data, as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (data-frame)\" )\n",
    "filename = \"raw/data_ml.csv\"\n",
    "LOG( f\"Reading {filename} [20 seconds]\" )\n",
    "d = pd.read_csv(filename)\n",
    "d['date'] = pd.to_datetime( d['date'] )\n",
    "\n",
    "predictors = list( signs.keys() )\n",
    "target = 'R1M_Usd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than a data-frame, it is often easier to have a list of matrices, one per variable, \n",
    "with one row per stock and one column per date: we can easily combine them or apply some function to each row, or each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (list of matrices)\" )\n",
    "LOG( \"Read data/data_ml.pickle\" )\n",
    "dd = load( \"data/data_ml.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single signals\n",
    "For any of the input variables, we can divide the universe into quintiles, long the top quintile, and short the bottom quintile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Backtest a single signal\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = predictors[0]\n",
    "LOG( f\"  {signal}\" )\n",
    "x = dd[signal].copy() * signs[signal]\n",
    "x.fillna(.5, inplace=True)  ## Replace missing values with the median (0.5, since the predictors are uniform)\n",
    "x = np.where( dd['universe'], 1, np.nan ) * x   # Only invest in stocks in the universe\n",
    "r = signal_backtest(x, y, date = DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( r['dates'], r['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(signal if signs[signal]>=0 else f\"- {signal}\")\n",
    "ax.text(0.02, .97, f\"μ={100*r['performance'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*r['performance'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={r['performance'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(f'plots/signal_wealth_{signal}.pdf')\n",
    "plt.show()\n",
    "\n",
    "r['performance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "We try to forecast the 1-month forward return from all the input variables. \n",
    "\n",
    "As we let the scale of the $L^1$ penalty vary, we have a family of increasingly complex models, from the intercept-only one, to the (unpenalized) least-squares model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Lasso\" )\n",
    "alphas, coef, _ = lasso_path( X = x, y = y, max_iter = 10_000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "LOG( \"Loop: backtests of the lasso models [5 minutes]\" )\n",
    "res = {}\n",
    "for j in tqdm(range(len(alphas))):\n",
    "    signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "    for i,predictor in enumerate(predictors):\n",
    "        signal += coef[i,j] * dd[predictor].fillna(.5)\n",
    "    signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "    r = signal_backtest(signal, y, date=DATE1)\n",
    "    r['performance'  ]['alpha'] = alphas[j]\n",
    "    r['in-sample'    ]['alpha'] = alphas[j]\n",
    "    r['out-of-sample']['alpha'] = alphas[j]\n",
    "    r['performance'  ]['period'] = 'all'\n",
    "    r['in-sample'    ]['period'] = 'in-sample'\n",
    "    r['out-of-sample']['period'] = 'out-of-sample'    \n",
    "    res[ f\"{alphas[j]} all\" ] = r['performance']\n",
    "    res[ f\"{alphas[j]} in\"  ] = r['in-sample']\n",
    "    res[ f\"{alphas[j]} out\" ] = r['out-of-sample']\n",
    "    \n",
    "    if j == 20:\n",
    "        # Wealth curves\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(6):\n",
    "            ax.plot( r['dates'], r['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "        ax.set_yscale('log')\n",
    "        ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )        \n",
    "        ax.set_title( f\"alpha={alphas[j]:5.3}\")\n",
    "        ax.text(0.02, .97, f\"μ={100*r['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "        ax.text(0.02, .90, f\"σ={100*r['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "        ax.text(0.02, .83, f\"IR={r['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "        fig.savefig(f\"plots/lasso_wealth_{j}.pdf\")\n",
    "        plt.show()\n",
    "        display( r['out-of-sample'] )\n",
    "        \n",
    "res = pd.concat( res.values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lasso(coefs, n=20, ax=None):\n",
    "    assert isinstance( coefs, pd.DataFrame ), f\"Expecting a DataFrame of coefficients, with one row per predictor, got a {type(coefs)}\"\n",
    "\n",
    "    ## Reorder the coefficients\n",
    "    def f(u):\n",
    "        v = np.argwhere(u != 0)\n",
    "        v = v.flatten()\n",
    "        if len(v) == 0:\n",
    "            return len(u)\n",
    "        return v[0]\n",
    "    i = np.apply_along_axis( f, 1, coefs )\n",
    "    i = np.argsort(i)\n",
    "    coefs = coefs.iloc[i,:]\n",
    "\n",
    "    import copy\n",
    "    cmap = copy.copy(matplotlib.cm.get_cmap(\"RdBu\"))    \n",
    "    cmap.set_bad('white')    # Plot the zeros as \"white\", not \"grey\" (after replacing them with nan)\n",
    "\n",
    "    ax_was_None = ax is None\n",
    "    if ax_was_None:\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "    tmp = coefs.iloc[:n,:].copy()\n",
    "    m = np.max(np.abs(tmp.values))\n",
    "    tmp[ tmp == 0 ] = np.nan\n",
    "    ax.imshow(tmp, vmin=-m, vmax=m, cmap=cmap, aspect='auto', interpolation='none')\n",
    "    ax.set_yticks(range(tmp.shape[0]))\n",
    "    ax.set_yticklabels( tmp.index )\n",
    "    ax.axes.yaxis.set_ticks_position(\"right\")\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    if ax_was_None:\n",
    "        fig.tight_layout()\n",
    "        plt.show()   \n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_lasso(pd.DataFrame( coef, index=predictors ), ax = ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/lasso_coefs.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the in-sample performance to increase, and the out-of-sample performance to increase and then decrease -- \n",
    "this is not what we see here: the out-of-sample performance does not decrease, and a linear model performs extremely well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'in-sample' )\n",
    "i2 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'out-of-sample' )\n",
    "i3 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'all' )\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "for j,key in enumerate(['Information Ratio', 'CAGR', 'Annualized Volatility']):\n",
    "    ax = axs[j]\n",
    "    ax.plot( res[i1]['alpha'], res[i1][key], label = \"in-sample\" )\n",
    "    ax.plot( res[i2]['alpha'], res[i2][key], label = \"out-of-sample\" )\n",
    "    #ax.plot( res[i3]['alpha'], res[i3][key], label = \"all\" )\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_xscale('log')\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_title(key)\n",
    "    ax.legend()\n",
    "fig.savefig(\"plots/lasso_regularization_path.pdf\")\n",
    "plt.show()\n",
    "\n",
    "number_of_predictors = ( coef != 0 ).sum(axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( alphas, number_of_predictors )\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_xscale('log')\n",
    "ax.invert_xaxis()\n",
    "ax.set_title('Number of predictors')\n",
    "fig.savefig(\"plots/lasso_number_of_predictors.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the linear models, to prevent it from overfitting the data, \n",
    "and to ensure the model remains interpretable, we can add constraints on the signs of the coefficients of the regression.\n",
    "Indeed we know in advance whether each predictor has a positive or negative impact on future returns:\n",
    "for instance, volatility has a negative impact, while earnings yield has a positive impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Fit the model\" )\n",
    "r = lsq_linear(\n",
    "    x, y,\n",
    "    (\n",
    "        [ 0      if signs[u] > 0 else -np.inf for u in predictors ],\n",
    "        [ np.inf if signs[u] > 0 else 0       for u in predictors ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# It is not supposed to be that sparse: usually, 2/3 of the coefficients are non-zero...\n",
    "w = np.round( 1e6 * r.x )\n",
    "{ p: w[i] for i,p in enumerate(predictors) if w[i] != 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += r.x[i] * dd[predictor].fillna(.5)\n",
    "    #signal += r_rev.x[i] * dd[predictor].fillna(.5)\n",
    "    #signal += r_ols.x[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Constrained regression')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/constrained_regression_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: linear model\n",
    "This is equivalent to the models above, but implemented with a neural network.\n",
    "* Linear model\n",
    "* Change the loss function\n",
    "* Add mini-batches\n",
    "* Add sign constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; forecast the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data: as above \n",
    "\n",
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Model\" )\n",
    "\n",
    "class Linear1(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear1,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "x = torch.tensor(x.values,               dtype=torch.float32)\n",
    "y = torch.tensor(y.values.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "model = Linear1(x.shape[1])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [2 minutes]\" )\n",
    "for t in tqdm(range(5000)):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Liner model (via pytorch)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model1_linear_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; data as id×signal×date array; forecast the returns\n",
    "\n",
    "Same model as above, but the data is no longer in an (id,date)×signal (2-dimensional) table, but in an id×date×signal 3-dimensional array.\n",
    "\n",
    "The model and the performance are similar but, for some reason, fitting the model is much more time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data (3-dimensional array)\" )\n",
    "\n",
    "def get_data_3(all=False, flip_signs=False):\n",
    "    \"\"\"\n",
    "    Data, as tensors.\n",
    "    Arguments: all: whether ti return all the data or just the training data\n",
    "    Returns:   x: id×date×signal\n",
    "               y: id×date, forward ratio returns\n",
    "               universe: id×date, 0 or 1\n",
    "    Global variables used: d, target, predictors, DATE1\n",
    "    \"\"\"\n",
    "\n",
    "    train = data_frame_to_list(d, id_name = 'stock_id', date_name = 'date')\n",
    "    if all:\n",
    "        i = np.array( [ True for u in train[ predictors[0] ].columns ] )\n",
    "    else:\n",
    "        i = np.array([ str(u) < DATE1 for u in train[ predictors[0] ].columns ])\n",
    "    train = { k: v.T[i].T for k,v in train.items() }\n",
    "        #x = train[ predictors ]\n",
    "        #y = np.log1p(train[ target ])\n",
    "    y = train[target]\n",
    "    if flip_signs: \n",
    "        x = [ signs[k] * train[k] for k in predictors ]\n",
    "    else:\n",
    "        x = [ train[k] for k in predictors ]\n",
    "    y = y.values\n",
    "    x = np.stack( [ u.values for u in x ] )\n",
    "\n",
    "    y = np.log1p(y)\n",
    "    universe = np.where( np.isfinite(y), 1, 0 )\n",
    "    # x = x.fillna(.5)\n",
    "    x = np.nan_to_num(x, nan=.5)\n",
    "    y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "    n = y.shape[0]   # Number of stocks\n",
    "    l = y.shape[1]   # Number of dates\n",
    "    k = x.shape[0]   # Number of predictors\n",
    "    assert k == len(predictors)\n",
    "    assert n == x.shape[1]\n",
    "    assert l == x.shape[2]\n",
    "\n",
    "    # x is now: signal×id×date\n",
    "\n",
    "    x = x.transpose([1,2,0])\n",
    "    assert x.shape == (n,l,k) # id×date×signal\n",
    "    assert y.shape == (n,l)   # id×date\n",
    "    assert universe.shape == (n,l)\n",
    "\n",
    "    if False: \n",
    "        x = x[:5,:4,:3]\n",
    "        y = y[:5,:4]\n",
    "        universe = universe = universe[:5,:4]\n",
    "\n",
    "    assert ( ~ np.isfinite(x) ).sum() == 0\n",
    "    assert ( ~ np.isfinite(y) ).sum() == 0\n",
    "    \n",
    "    return x, y, universe\n",
    "\n",
    "x, y, universe = get_data_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear2,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        y = y[:,:,0]         #  n×l\n",
    "        return y\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear2(x.shape[2])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [LONG: 20 minutes for 5000 epochs]\" )\n",
    "N = 5000\n",
    "losses = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    y_pred = model(x) * universe[:,:,0]\n",
    "    loss = criterion(y_pred,y[:,:,0])\n",
    "    losses[t] = loss.item()\n",
    "    pbar.set_description( f\"Loss={loss.item():.5f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( losses )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model2_linear_3d_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the resulting strategy (exactly the same code as above)\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Linear model (via pytorch)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model2_linear_3d_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple, nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not really work: the optimization often remains stuck. \n",
    "Restarting the optimization several (10?) times eventually gives something acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(NonLinear2,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,x):\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y); # n×l×1\n",
    "        y = y[:,:,0]         #  n×l\n",
    "        return y\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = NonLinear2(x.shape[2])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [LONG: 20 minutes for 5000 epochs]\" )\n",
    "N = 5000\n",
    "losses = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    y_pred = model(x) * universe[:,:,0]\n",
    "    loss = criterion(y_pred,y[:,:,0])\n",
    "    losses[t] = loss.item()\n",
    "    pbar.set_description( f\"Loss={loss.item():.5f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( losses )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model2_nonlinear_3d_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model(x).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Non-linear')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model2_nonlinear_3d_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute portfolio weights; optimize the information ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear3(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear3,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        p = y.exp()             # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p\n",
    "    \n",
    "model = Linear3(x.shape[2])\n",
    "# model( (x,universe) ).detach().numpy().sum(axis=0)   # Should sum to 1 for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to maximize the IR\n",
    "\n",
    "LOG( \"[LONG] 50 minutes for 10,000 epochs\" )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    w = model( (x,universe) )\n",
    "    ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={IR.item():.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( IRs )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model3_linear_IR_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model3_linear_IR_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "s = analyze_returns( r[ r.index > DATE1 ], as_df = True )\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights)')\n",
    "ax.text(0.02, .97, f\"μ={100*s.iloc[0,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*s.iloc[0,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={s.iloc[0,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model3_linear_IR_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batches\n",
    "To make the model more robust, we train it on \"mini-batches\", defined by a random subset of stocks, and a time interval (contiguous dates).\n",
    "These are not real mini-batches, because the loss function is not a sum over the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 30 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear3(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (signal, mini-batches)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "s = analyze_returns( r[ r.index > DATE1 ], as_df = True )\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights, mini-batches)')\n",
    "ax.text(0.02, .97, f\"μ={100*s.iloc[0,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*s.iloc[0,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={s.iloc[0,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-short strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear4(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear4,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        y = y * universe\n",
    "        long  = torch.where( y > 0,  y, torch.zeros(1) )\n",
    "        short = torch.where( y < 0, -y, torch.zeros(1) )\n",
    "        long  = long [:,:,0]  #  n×l\n",
    "        short = short[:,:,0]\n",
    "        long  = long  / ( 1e-6 +  long.sum(axis=0) )\n",
    "        short = short / ( 1e-6 + short.sum(axis=0) )\n",
    "        p = long - short\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same code as above\n",
    "\n",
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear4(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole training sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (long-short, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "s = analyze_returns( r[ r.index > DATE1 ], as_df = True )   # Very bad...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (long-short, weights)')\n",
    "ax.text(0.02, .97, f\"μ={100*s.iloc[0,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*s.iloc[0,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={s.iloc[0,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Add transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear6(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear6,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x)\n",
    "        y = self.fc2(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear6(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model6_linear_IR_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (deep linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model6_linear_IR_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "s = analyze_returns( r[ r.index > DATE1 ], as_df = True )   \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (deep linear, weights)')\n",
    "ax.text(0.02, .97, f\"μ={100*s.iloc[0,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*s.iloc[0,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={s.iloc[0,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model6_linear_IR_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear6(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(NonLinear6,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = NonLinear6(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model6_nonlinear_IR_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model6_nonlinear_IR_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "s = analyze_returns( r[ r.index > DATE1 ], as_df = True )   \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights)')\n",
    "ax.text(0.02, .97, f\"μ={100*s.iloc[0,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*s.iloc[0,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={s.iloc[0,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model6_nonlinear_IR_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to understand what the model actually learned\n",
    "\n",
    "def plot_copula(x,y, ax=None, title=None, unif=True, cmap='Blues'):\n",
    "    i = np.isfinite(x) & np.isfinite(y)\n",
    "    x = x[i]\n",
    "    y = y[i]\n",
    "\n",
    "    if unif:\n",
    "        x = uniformize(x)\n",
    "        y = uniformize(y)\n",
    "        xmin, xmax, ymin, ymax = 0,1, 0,1\n",
    "    else:\n",
    "        xmin, xmax = min(x), max(x)\n",
    "        ymin, ymax = min(y), max(y)\n",
    "\n",
    "    ax_was_None = ax is None\n",
    "    if ax_was_None:\n",
    "        fig, ax = plt.subplots( figsize = (4,4) )\n",
    "\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    values = np.vstack([x,y])\n",
    "    kernel = scipy.stats.gaussian_kde(values)\n",
    "    f = np.reshape( kernel(positions).T, xx.shape )\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    cfset = ax.contourf(xx, yy, f, cmap=cmap)\n",
    "    cset = ax.contour(xx, yy, f, colors='k')\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    if ax_was_None:\n",
    "        plt.show()\n",
    "        \n",
    "def remove_empty_axes(axs):\n",
    "    for ax in axs.flatten():\n",
    "        if (not ax.lines) and (not ax.collections) and (not ax.has_data()):\n",
    "            ax.axis('off')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Copula densities [VERY LONG: 1.5 hours]\")        \n",
    "nr, nc = mfrow(x.shape[2], aspect=1)\n",
    "fig, axs = plt.subplots( nr, nc, figsize=(29.7/1.5,21/1.5) )\n",
    "for i in tqdm(range(x.shape[2])):\n",
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    b = signal.values\n",
    "    b = np.apply_along_axis( uniformize, 0, b )    \n",
    "    ax = axs.flatten()[i]\n",
    "    plot_copula(a.flatten(),b.flatten(), ax = ax)\n",
    "    ax.set_title( predictors[i] )\n",
    "remove_empty_axes(axs)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=.2, wspace=.05)\n",
    "fig.savefig('plots/model6_nonlinear_copulas_all.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual plots\n",
    "for i in tqdm(range(x.shape[2])):\n",
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    b = signal.values\n",
    "    b = np.apply_along_axis( uniformize, 0, b )    \n",
    "    fig, ax = plt.subplots()\n",
    "    plot_copula(a.flatten(),b.flatten(), ax = ax)\n",
    "    ax.set_title( predictors[i] )\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'plots/model6_nonlinear_copulas_{predictors[i]}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = mfrow(x.shape[2], aspect=1)\n",
    "fig, axs = plt.subplots( nr, nc, figsize=(29.7/1.5,21/1.5) )\n",
    "for i in tqdm(range(x.shape[2])):\n",
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    \n",
    "    if False: \n",
    "      # Some of the variables just measure size: normalize them\n",
    "      # (not really possible with the data we have: we can only see something for FCF -- for most other variables, the effect of the MCap is overpowering)\n",
    "      if i > 0:\n",
    "        a = a - x.detach().numpy()[:,:,0]\n",
    "        a = np.apply_along_axis( uniformize, 0, a )    \n",
    "        \n",
    "    b = signal.values.copy()\n",
    "    a = np.floor( a * 20 * .9999 )\n",
    "    b = np.apply_along_axis( uniformize, 0, b )\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    c = pd.DataFrame( { 'quantile': a, 'value': b } )\n",
    "    c = c.pivot_table(values='value', columns = 'quantile', aggfunc='mean')\n",
    "    ax = axs.flatten()[i]\n",
    "    ax.plot( c.columns, c.values.flatten() )\n",
    "    ax.scatter( c.columns, c.values.flatten() )\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)    \n",
    "    ax.set_title( predictors[i] )\n",
    "remove_empty_axes(axs)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=.2, wspace=.05)\n",
    "fig.savefig('plots/model6_nonlinear_median_per_quantile_all.pdf')\n",
    "plt.show()\n",
    "\n",
    "LOG( \"Individual plots\" )\n",
    "for i in tqdm(range(x.shape[2])):\n",
    "   \n",
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    b = signal.values.copy()\n",
    "    a = np.floor( a * 20 * .9999 )\n",
    "    b = np.apply_along_axis( uniformize, 0, b )\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    c = pd.DataFrame( { 'quantile': a, 'value': b } )\n",
    "    c = c.pivot_table(values='value', columns = 'quantile', aggfunc='mean')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot( c.columns, c.values.flatten() )\n",
    "    ax.scatter( c.columns, c.values.flatten() )\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)    \n",
    "    ax.set_title( predictors[i] )\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'plots/model6_nonlinear_median_per_quantile_{predictors[i]}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = mfrow(x.shape[2])\n",
    "fig, axs = plt.subplots( nr, nc, figsize=(20,20) )\n",
    "for i in tqdm(range(x.shape[2])):\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    b = signal.values.copy()\n",
    "    a = np.floor( a * 20 * .9999 )\n",
    "    b = np.apply_along_axis( uniformize, 0, b )\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    c = pd.DataFrame( { 'quantile': a, 'value': b } )\n",
    "    c1 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 25))\n",
    "    c2 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 50))\n",
    "    c3 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 75))\n",
    "    c0 = c1.columns\n",
    "    c1 = c1.values.flatten()\n",
    "    c2 = c2.values.flatten()\n",
    "    c3 = c3.values.flatten()\n",
    "    ax.fill_between( c0, c1, c3, color='lightblue')\n",
    "    ax.plot(c0, c1, color='tab:blue')\n",
    "    ax.plot(c0, c3, color='tab:blue')\n",
    "    ax.plot(c0, c2, marker='o', color='black')\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)    \n",
    "    ax.set_title( predictors[i] )\n",
    "remove_empty_axes(axs)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=.2, wspace=.05)\n",
    "fig.savefig('plots/model6_nonlinear_quartiles_per_quantile_all.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "    b = signal.values.copy()\n",
    "    a = np.floor( a * 20 * .9999 )\n",
    "    b = np.apply_along_axis( uniformize, 0, b )\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    c = pd.DataFrame( { 'quantile': a, 'value': b } )\n",
    "    c1 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 25))\n",
    "    c2 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 50))\n",
    "    c3 = c.pivot_table(values='value', columns = 'quantile', aggfunc=lambda x: np.percentile(x, 75))\n",
    "    c0 = c1.columns\n",
    "    c1 = c1.values.flatten()\n",
    "    c2 = c2.values.flatten()\n",
    "    c3 = c3.values.flatten()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between( c0, c1, c3, color='lightblue')\n",
    "    ax.plot(c0, c1, marker='o', color='tab:blue')\n",
    "    ax.plot(c0, c3, marker='o', color='tab:blue')\n",
    "    ax.plot(c0, c2, marker='o', color='black')\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)    \n",
    "    ax.set_title( predictors[i] )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for which in ['scatter', 'hexbin']:\n",
    "    LOG( which )\n",
    "    nr, nc = mfrow(x.shape[2], aspect=1)\n",
    "    fig, axs = plt.subplots( nr, nc, figsize=(29.7/1.5,21/1.5) )\n",
    "    for i in tqdm(range(x.shape[2])):\n",
    "        a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "        b = signal.values.copy()\n",
    "        b = np.apply_along_axis( uniformize, 0, b )\n",
    "        a = a.flatten()\n",
    "        b = b.flatten()\n",
    "        ax = axs.flatten()[i]\n",
    "        if which == 'scatter': \n",
    "            ax.scatter(\n",
    "                a + .01 * np.random.uniform(-1,1,n), \n",
    "                b + .01 * np.random.uniform(-1,1,n),\n",
    "                alpha=1/255, \n",
    "                s=10\n",
    "            )\n",
    "        else: \n",
    "            ax.hexbin( a, b, gridsize=20, cmap='Blues' )\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)    \n",
    "        ax.set_title( predictors[i] )\n",
    "    remove_empty_axes(axs)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=.22, wspace=.05)\n",
    "    if which == 'scatter':\n",
    "        LOG( f\"{which}: PNG file\" )\n",
    "        fig.savefig(f'plots/model6_nonlinear_{which}_all.png', facecolor='white', transparent=False)  # The PDF file would be too large...\n",
    "    else: \n",
    "        LOG( f\"{which}: PDF file\" )\n",
    "        fig.savefig(f'plots/model6_nonlinear_{which}_all.pdf')\n",
    "    LOG( \"Plot\" )\n",
    "    plt.show()\n",
    "    \n",
    "    LOG( f\"{which}: Individual plots\" )\n",
    "    for i in tqdm(range(x.shape[2])):\n",
    "\n",
    "        a = np.where( universe[:,:,0], x[:,:,i], np.nan )\n",
    "        b = signal.values.copy()\n",
    "        b = np.apply_along_axis( uniformize, 0, b )\n",
    "        a = a.flatten()\n",
    "        b = b.flatten()\n",
    "        if which == 'scatter': \n",
    "            fig, ax = plt.subplots(figsize=(19.20,10.80))\n",
    "        else: \n",
    "            fig, ax = plt.subplots()\n",
    "        if which == 'scatter': \n",
    "            n = len(a)\n",
    "            ax.scatter( \n",
    "                a + .01 * np.random.uniform(-1,1,n), \n",
    "                b + .01 * np.random.uniform(-1,1,n),\n",
    "                alpha=1/255, s=200\n",
    "            )\n",
    "            ax.set_xlim(0,1)\n",
    "            ax.set_ylim(0,1)            \n",
    "        else: \n",
    "            ax.hexbin( a, b, gridsize=20, cmap='Blues' )\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)    \n",
    "        ax.set_title( predictors[i] )        \n",
    "        fig.tight_layout()\n",
    "        if which == 'scatter':\n",
    "            fig.savefig(f'plots/model6_nonlinear_{which}_{predictors[i]}.png', facecolor='white', transparent = False)\n",
    "        else:\n",
    "            fig.savefig(f'plots/model6_nonlinear_{which}_{predictors[i]}.pdf')\n",
    "        plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: lattice networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive weights\n",
    "Simply constraint the weights to be positive is too restrictive: with ReLU nonlinearities, we end up with increasing *convex* functions.\n",
    "\n",
    "Convergence issues: nothing usable.\n",
    "I made the following changes: \n",
    "- Use an absolute value instead of a softplus to ensure the weights are positive\n",
    "- To make the weights positive, replace $\\exp y$ with $\\text{softplus}(y)$, and add $+10^{-6}$ lest all the weights be too close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPositive(torch.nn.Module):\n",
    "    def __init__(self, n, k): \n",
    "        super(LinearPositive, self).__init__()\n",
    "        self.layer = torch.nn.Linear(n,k)\n",
    "    def forward(self,x):\n",
    "        w = self.weight_actual()\n",
    "        y = F.linear(x,w) + self.layer.bias\n",
    "        return y\n",
    "    def weight_actual(self):\n",
    "        #return self.layer.weight\n",
    "        #return torch.nn.Softplus()( self.layer.weight )\n",
    "        return self.layer.weight.abs()\n",
    "    \n",
    "class Calibrator(torch.nn.Module):\n",
    "    def __init__(self, k=5, xmin=-1, xmax=+1):\n",
    "        assert k >= 2\n",
    "        super(Calibrator, self).__init__()\n",
    "        self.k = k\n",
    "        self.xs = torch.Tensor( np.linspace(xmin, xmax, k) )\n",
    "        ys = np.random.uniform(-1,1, size=k)\n",
    "        ys[0] = xmin\n",
    "        ys[1] = xmax\n",
    "        ys = sorted(ys)\n",
    "        ys = np.diff(ys)\n",
    "        ys = inverse_softplus(ys)\n",
    "        self.ys = torch.nn.Parameter( torch.Tensor( ys ) )\n",
    "    def ys_actual(self):\n",
    "        ys = self.ys\n",
    "        ys = F.softplus(ys)\n",
    "        ys = torch.cumsum(ys,0)\n",
    "        y = torch.zeros(self.k)\n",
    "        y[1:] = ys\n",
    "        y = y - 1\n",
    "        return y\n",
    "    def interpolate(self, xs, ys, x_new ):\n",
    "        ## k points, k-1 intervals between them, k+1 intervals if we include the infinite ones on the left and right\n",
    "        k = len(xs)\n",
    "        slopes = ( ys[1:] - ys[:-1] ) / ( xs[1:] - xs[:-1] )\n",
    "        #print( slopes )\n",
    "        current_slope = 0\n",
    "        X = torch.zeros( x_new.shape )\n",
    "        X = X + ys[0]\n",
    "        for i in range(k-1): \n",
    "            X = X + F.relu( x_new - xs[i] ) * ( slopes[i] - current_slope )\n",
    "            current_slope = slopes[i]\n",
    "            #print( current_slope )\n",
    "        X = X + F.relu( x_new - xs[k-1] ) * ( 0 - current_slope )\n",
    "        return X\n",
    "    def interpolate_test(self):\n",
    "        \"\"\"Check that my interpolation function works as it should\"\"\"\n",
    "        xs = torch.Tensor( np.linspace(0,1,6) )\n",
    "        ys = torch.Tensor( np.random.uniform( size=6 ) )\n",
    "        x_new = torch.Tensor( np.linspace(-.1,1.1,100) )\n",
    "        y_new = Calibrator().interpolate(xs, ys, x_new)    \n",
    "        x_new = x_new.detach().numpy()\n",
    "        y_new = y_new.detach().numpy()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot( x_new, y_new )\n",
    "        ax.scatter( xs.detach().numpy(), ys.detach().numpy() )\n",
    "        plt.show()\n",
    "    def forward(self, x):\n",
    "        return self.interpolate( self.xs, self.ys_actual(), x )    \n",
    "    \n",
    "def inverse_softplus(y):\n",
    "    x = np.log( np.exp(y) - 1 )\n",
    "    return np.where( y > 20, y, x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monotonic1(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Monotonic1,self).__init__()\n",
    "        self.fc1 = LinearPositive(k,16)\n",
    "        self.fc2 = LinearPositive(16,4)\n",
    "        self.fc3 = LinearPositive(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = torch.nn.Softplus()(y) + 1e-6  # Was: p=y.exp()\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 1h30min for 10,000 epochs; convergence issues\" )\n",
    "\n",
    "x, y, universe = get_data_3( flip_signs=True )\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Monotonic1(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 1_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model7_monotonic1_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True, flip_signs=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model7_monotonic1_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model did not converge, we often have an equal-weighted portfolio: check this is not the case\n",
    "i = np.argsort( - signal.sum(axis=1) )\n",
    "fig, ax = plt.subplots( figsize= (20,4) )\n",
    "ax.imshow(signal.iloc[i[:50],:], aspect='auto',cmap='Blues')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Stocks')\n",
    "ax.set_title(\"Stocks with the largest weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive weights + parametrized activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monotonic2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Monotonic2,self).__init__()\n",
    "        self.fc1 = LinearPositive(k,16)\n",
    "        self.fc2 = LinearPositive(16,4)\n",
    "        self.fc3 = LinearPositive(4,1)\n",
    "        self.f1 = Calibrator()\n",
    "        self.f2 = Calibrator()\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = self.f1(y)\n",
    "        y = self.fc2(y); y = self.f2(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = torch.nn.Softplus()(y) + 1e-6  # Was: p=y.exp()\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 1h30min for 10,000 epochs; convergence issues\" )\n",
    "\n",
    "x, y, universe = get_data_3( flip_signs=True )\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Monotonic2(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 5_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model8_monotonic2_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True, flip_signs=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model8_monotonic2_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the activation functions learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty for non-monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This does not work. Try to first estimate a nonlinear model without the penalty (that should work), and only then add the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just the nonlinear model above\n",
    "## Differences: \n",
    "## - Also return the unnormalized score (we want the derivative, and the weights sum to 1)\n",
    "## - requires_grad=True on x\n",
    "## - Penalty term added to the loss function\n",
    "\n",
    "class Monotonic3(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Monotonic3,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p, y * universe    # Also return the unnormalized score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32, requires_grad = True)   ## For the penalty, we will need the gradient wrt x\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Monotonic3(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs       = np.nan * np.zeros(N)\n",
    "penalties = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    input = x[i,:,:][:,j,:]\n",
    "    \n",
    "    w, yhat = model( (input, universe[i,:,:][:,j,:]) )\n",
    "    \n",
    "    ## Penalty\n",
    "    df = torch.autograd.grad(outputs = yhat.sum(), inputs = input, retain_graph = True, create_graph = True )[0]\n",
    "    penalty = 1e3 * (df.clip(max=0)**2).mean()\n",
    "\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR + penalty\n",
    "    \n",
    "    IRs[t] = IR.item()\n",
    "    penalties[t] = penalty.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f} Penalty={penalties[t]:.6f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model9_monotonic3_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal, _ = model( (x,universe) )\n",
    "signal = signal.detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model9_monotonic3_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import torch \n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "class OptimalPortfolio(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(OptimalPortfolio,self).__init__()\n",
    "        self.fc = torch.nn.Linear(k,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return p, y * universe    # Also return the unnormalized score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
