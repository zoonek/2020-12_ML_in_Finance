{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from parameters import *\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lasso_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data, as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( np.unique( d['stock_id'] ) )\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (data-frame)\" )\n",
    "filename = \"raw/data_ml.csv\"\n",
    "LOG( f\"Reading {filename} [20 seconds]\" )\n",
    "d = pd.read_csv(filename)\n",
    "d['date'] = pd.to_datetime( d['date'] )\n",
    "\n",
    "predictors = list( signs.keys() )\n",
    "target = 'R1M_Usd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than a data-frame, it is often easier to have a list of matrices, one per variable, \n",
    "with one row per stock and one column per date: we can easily combine them or apply some function to each row, or each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (list of matrices)\" )\n",
    "LOG( \"Read data/data_ml.pickle\" )\n",
    "dd = load( \"data/data_ml.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single signals\n",
    "For any of the input variables, we can divide the universe into quintiles, long the top quintile, and short the bottom quintile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Backtest a single signal\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = predictors[0]\n",
    "LOG( f\"  {signal}\" )\n",
    "x = dd[signal].copy() * signs[signal]\n",
    "x.fillna(.5, inplace=True)  ## Replace missing values with the median (0.5, since the predictors are uniform)\n",
    "x = np.where( dd['universe'], 1, np.nan ) * x   # Only invest in stocks in the universe\n",
    "r = signal_backtest(x, y, date = DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( r['dates'], r['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(signal if signs[signal]>=0 else f\"- {signal}\")\n",
    "fig.savefig(f'plots/signal_wealth_{signal}.pdf')\n",
    "plt.show()\n",
    "\n",
    "r['performance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "We try to forecast the 1-month forward return from all the input variables. \n",
    "\n",
    "As we let the scale of the $L^1$ penalty vary, we have a family of increasingly complex models, from the intercept-only one, to the (unpenalized) least-squares model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Lasso\" )\n",
    "alphas, coef, _ = lasso_path( X = x, y = y, max_iter = 10_000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "LOG( \"Loop: backtests of the lasso models [5 minutes]\" )\n",
    "res = {}\n",
    "for j in tqdm(range(len(alphas))):\n",
    "    signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "    for i,predictor in enumerate(predictors):\n",
    "        signal += coef[i,j] * dd[predictor].fillna(.5)\n",
    "    signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "    r = signal_backtest(signal, y, date=DATE1)\n",
    "    r['performance'  ]['alpha'] = alphas[j]\n",
    "    r['in-sample'    ]['alpha'] = alphas[j]\n",
    "    r['out-of-sample']['alpha'] = alphas[j]\n",
    "    r['performance'  ]['period'] = 'all'\n",
    "    r['in-sample'    ]['period'] = 'in-sample'\n",
    "    r['out-of-sample']['period'] = 'out-of-sample'    \n",
    "    res[ f\"{alphas[j]} all\" ] = r['performance']\n",
    "    res[ f\"{alphas[j]} in\"  ] = r['in-sample']\n",
    "    res[ f\"{alphas[j]} out\" ] = r['out-of-sample']\n",
    "    \n",
    "    if j == 20:\n",
    "        # Wealth curves\n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(6):\n",
    "            ax.plot( r['dates'], r['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "        ax.set_yscale('log')\n",
    "        ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )        \n",
    "        ax.set_title( f\"alpha={alphas[j]:5.3}\")\n",
    "        fig.savefig(f\"plots/lasso_wealth_{j}.pdf\")\n",
    "        plt.show()\n",
    "        \n",
    "res = pd.concat( res.values() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the in-sample performance to increase, and the out-of-sample performance to increase and then decrease -- \n",
    "this is not what we see here: the out-of-sample performance does not decrease, and a linear model performs extremely well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'in-sample' )\n",
    "i2 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'out-of-sample' )\n",
    "i3 = ( res['portfolio'] == 'LS' ) & ( res['period'] == 'all' )\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "for j,key in enumerate(['Information Ratio', 'CAGR', 'Annualized Volatility']):\n",
    "    ax = axs[j]\n",
    "    ax.plot( res[i1]['alpha'], res[i1][key], label = \"in-sample\" )\n",
    "    ax.plot( res[i2]['alpha'], res[i2][key], label = \"out-of-sample\" )\n",
    "    #ax.plot( res[i3]['alpha'], res[i3][key], label = \"all\" )\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_xscale('log')\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_title(key)\n",
    "    ax.legend()\n",
    "fig.savefig(\"plots/lasso_regularization_path.pdf\")\n",
    "plt.show()\n",
    "\n",
    "number_of_predictors = ( coef != 0 ).sum(axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( alphas, number_of_predictors )\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_xscale('log')\n",
    "ax.invert_xaxis()\n",
    "ax.set_title('Number of predictors')\n",
    "fig.savefig(\"plots/lasso_number_of_predictors.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the linear models, to prevent it from overfitting the data, \n",
    "and to ensure the model remains interpretable, we can add constraints on the signs of the coefficients of the regression.\n",
    "Indeed we know in advance whether each predictor has a positive or negative impact on future returns:\n",
    "for instance, volatility has a negative impact, while earnings yield has a positive impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Fit the model\" )\n",
    "r = lsq_linear(\n",
    "    x, y,\n",
    "    (\n",
    "        [ 0      if signs[u] > 0 else -np.inf for u in predictors ],\n",
    "        [ np.inf if signs[u] > 0 else 0       for u in predictors ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# It is not supposed to be that sparse: usually, 2/3 of the coefficients are non-zero...\n",
    "w = np.round( 1e6 * r.x )\n",
    "{ p: w[i] for i,p in enumerate(predictors) if w[i] != 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += r.x[i] * dd[predictor].fillna(.5)\n",
    "    #signal += r_rev.x[i] * dd[predictor].fillna(.5)\n",
    "    #signal += r_ols.x[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Constrained regression')\n",
    "fig.savefig(\"plots/constrained_regression_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: linear model\n",
    "This is equivalent to the models above, but implemented with a neural network.\n",
    "* Linear model\n",
    "* Change the loss function\n",
    "* Add mini-batches\n",
    "* Add sign constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; forecast the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data: as above \n",
    "\n",
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Model\" )\n",
    "\n",
    "class Linear1(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear1,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "x = torch.tensor(x.values,               dtype=torch.float32)\n",
    "y = torch.tensor(y.values.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "model = Linear1(x.shape[1])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop\" )\n",
    "for t in tqdm(range(5000)):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Liner model (via pytorch)')\n",
    "fig.savefig(\"plots/model1_linear_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; data as id×signal×date array; forecast the returns\n",
    "\n",
    "Same model as above, but the data is no longer in an (id,date)×signal (2-dimensional) table, but in an id×date×signal 3-dimensional array.\n",
    "\n",
    "The model and the performance are similar but, for some reason, fitting the model is much more time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data (3-dimensional array)\" )\n",
    "\n",
    "def get_data_3(all=False):\n",
    "    \"\"\"\n",
    "    Data, as tensors.\n",
    "    Arguments: all: whether ti return all the data or just the training data\n",
    "    Returns:   x: id×date×signal\n",
    "               y: id×date, forward ratio returns\n",
    "               universe: id×date, 0 or 1\n",
    "    Global variables used: d, target, predictors, DATE1\n",
    "    \"\"\"\n",
    "\n",
    "    train = data_frame_to_list(d, id_name = 'stock_id', date_name = 'date')\n",
    "    if all:\n",
    "        i = np.array( [ True for u in train[ predictors[0] ].columns ] )\n",
    "    else:\n",
    "        i = np.array([ str(u) < DATE1 for u in train[ predictors[0] ].columns ])\n",
    "    train = { k: v.T[i].T for k,v in train.items() }\n",
    "        #x = train[ predictors ]\n",
    "        #y = np.log1p(train[ target ])\n",
    "    y = train[target]\n",
    "    x = [ train[k] for k in predictors ]\n",
    "    y = y.values\n",
    "    x = np.stack( [ u.values for u in x ] )\n",
    "\n",
    "    y = np.log1p(y)\n",
    "    universe = np.where( np.isfinite(y), 1, 0 )\n",
    "    # x = x.fillna(.5)\n",
    "    x = np.nan_to_num(x, nan=.5)\n",
    "    y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "    n = y.shape[0]   # Number of stocks\n",
    "    l = y.shape[1]   # Number of dates\n",
    "    k = x.shape[0]   # Number of predictors\n",
    "    assert k == len(predictors)\n",
    "    assert n == x.shape[1]\n",
    "    assert l == x.shape[2]\n",
    "\n",
    "    # x is now: signal×id×date\n",
    "\n",
    "    x = x.transpose([1,2,0])\n",
    "    assert x.shape == (n,l,k) # id×date×signal\n",
    "    assert y.shape == (n,l)   # id×date\n",
    "    assert universe.shape == (n,l)\n",
    "\n",
    "    if False: \n",
    "        x = x[:5,:4,:3]\n",
    "        y = y[:5,:4]\n",
    "        universe = universe = universe[:5,:4]\n",
    "\n",
    "    assert ( ~ np.isfinite(x) ).sum() == 0\n",
    "    assert ( ~ np.isfinite(y) ).sum() == 0\n",
    "    \n",
    "    return x, y, universe\n",
    "\n",
    "x, y, universe = get_data_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear2,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        y = y[:,:,0]         #  n×l\n",
    "        return y\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear1(x.shape[2])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [LONG: 20 minutes for 5000 epochs]\" )\n",
    "N = 5000\n",
    "losses = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    y_pred = model(x) * universe\n",
    "    loss = criterion(y_pred,y)\n",
    "    losses[t] = loss.item()\n",
    "    pbar.set_description( f\"Loss={loss.item():.5f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( losses )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model2_linear_3d_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the resulting strategy (exactly the same code as above)\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Liner model (via pytorch)')\n",
    "fig.savefig(\"plots/model2_linear_3d_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute portfolio weights; optimize the information ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear3(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear3,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        p = y.exp()             # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p\n",
    "    \n",
    "model = Linear3(x.shape[2])\n",
    "# model( (x,universe) ).detach().numpy().sum(axis=0)   # Should sum to 1 for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to maximize the IR\n",
    "\n",
    "LOG( \"[LONG] 50 minutes for 10,000 epochs\" )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    w = model( (x,universe) )\n",
    "    ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={IR.item():.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( IRs )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model3_linear_IR_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (signal)')\n",
    "fig.savefig(\"plots/model3_linear_IR_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights)')\n",
    "fig.savefig(\"plots/model3_linear_IR_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "analyze_returns( r[ r.index > DATE1 ], as_df = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batches\n",
    "To make the model more robust, we train it on \"mini-batches\", defined by a random subset of stocks, and a time interval (contiguous dates).\n",
    "These are not real mini-batches, because the loss function is not a sum over the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 30 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear3(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (signal, mini-batches)')\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights, mini-batches)')\n",
    "fig.savefig(\"plots/model3b_linear_IR_minibatches_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "analyze_returns( r[ r.index > DATE1 ], as_df = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-short strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear4(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear4,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        y = y * universe\n",
    "        long  = torch.where( y > 0,  y, torch.zeros(1) )\n",
    "        short = torch.where( y < 0, -y, torch.zeros(1) )\n",
    "        long  = long [:,:,0]  #  n×l\n",
    "        short = short[:,:,0]\n",
    "        long  = long  / ( 1e-6 +  long.sum(axis=0) )\n",
    "        short = short / ( 1e-6 + short.sum(axis=0) )\n",
    "        p = long - short\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same code as above\n",
    "\n",
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear4(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole training sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (long-short, signal)')\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (long-short, weights)')\n",
    "fig.savefig(\"plots/model4_linear_IR_LS_wealth_weights.pdf\")\n",
    "plt.show()\n",
    "\n",
    "analyze_returns( r[ r.index > DATE1 ], as_df = True )   # Very bad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Add transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear6(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear6,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x)\n",
    "        y = self.fc2(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Linear6(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (deep linear, signal)')\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (deep linear, weights)')\n",
    "plt.show()\n",
    "\n",
    "analyze_returns( r[ r.index > DATE1 ], as_df = True )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear6(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(NonLinear6,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3()\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = NonLinear6(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the strategy actually learned\n",
    "\n",
    "r = compute_portfolio_returns( signal, np.expm1(trailing_log_returns) ) \n",
    "p = np.exp(cumsum_na(r))               # Log-price = cummulated log-returns\n",
    "p = replace_last_leading_NaN_with_1(p) # \"cumsum\" is not the exact inverse of \"diff\" -- it discards the first value, 1: put it back\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(5):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.plot( p.index, p, color='black' )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (weights)')\n",
    "plt.show()\n",
    "\n",
    "analyze_returns( r[ r.index > DATE1 ], as_df = True )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: lattice networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
