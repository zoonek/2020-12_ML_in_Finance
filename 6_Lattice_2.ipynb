{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from functions2 import *\n",
    "from parameters import *\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lasso_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (data-frame)\" )\n",
    "filename = \"raw/data_ml.csv\"\n",
    "LOG( f\"  Reading {filename} [20 seconds]\" )\n",
    "d = pd.read_csv(filename)\n",
    "d['date'] = pd.to_datetime( d['date'] )\n",
    "\n",
    "predictors = list( signs.keys() )\n",
    "target = 'R1M_Usd'\n",
    "\n",
    "LOG( \"Data (list of matrices)\" )\n",
    "LOG( \"  Reading data/data_ml.pickle\" )\n",
    "dd = load( \"data/data_ml.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: lattice networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPositive(torch.nn.Module):\n",
    "    def __init__(self, n, k): \n",
    "        super(LinearPositive, self).__init__()\n",
    "        self.layer = torch.nn.Linear(n,k)\n",
    "    def forward(self,x):\n",
    "        w = self.weight_actual()\n",
    "        y = F.linear(x,w) + self.layer.bias\n",
    "        return y\n",
    "    def weight_actual(self):\n",
    "        #return self.layer.weight\n",
    "        #return torch.nn.Softplus()( self.layer.weight )\n",
    "        return self.layer.weight.abs()\n",
    "    \n",
    "class Calibrator(torch.nn.Module):\n",
    "    def __init__(self, k=5, xmin=-1, xmax=+1):\n",
    "        assert k >= 2\n",
    "        super(Calibrator, self).__init__()\n",
    "        self.k = k\n",
    "        self.xs = torch.Tensor( np.linspace(xmin, xmax, k) )\n",
    "        ys = np.random.uniform(-1,1, size=k)\n",
    "        ys[0] = xmin\n",
    "        ys[1] = xmax\n",
    "        ys = sorted(ys)\n",
    "        ys = np.diff(ys)\n",
    "        ys = inverse_softplus(ys)\n",
    "        self.ys = torch.nn.Parameter( torch.Tensor( ys ) )\n",
    "    def ys_actual(self):\n",
    "        ys = self.ys\n",
    "        ys = F.softplus(ys)\n",
    "        ys = torch.cumsum(ys,0)\n",
    "        y = torch.zeros(self.k)\n",
    "        y[1:] = ys\n",
    "        y = y - 1\n",
    "        return y\n",
    "    def interpolate(self, xs, ys, x_new ):\n",
    "        ## k points, k-1 intervals between them, k+1 intervals if we include the infinite ones on the left and right\n",
    "        k = len(xs)\n",
    "        slopes = ( ys[1:] - ys[:-1] ) / ( xs[1:] - xs[:-1] )\n",
    "        #print( slopes )\n",
    "        current_slope = 0\n",
    "        X = torch.zeros( x_new.shape )\n",
    "        X = X + ys[0]\n",
    "        for i in range(k-1): \n",
    "            X = X + F.relu( x_new - xs[i] ) * ( slopes[i] - current_slope )\n",
    "            current_slope = slopes[i]\n",
    "            #print( current_slope )\n",
    "        X = X + F.relu( x_new - xs[k-1] ) * ( 0 - current_slope )\n",
    "        return X\n",
    "    def interpolate_test(self):\n",
    "        \"\"\"Check that my interpolation function works as it should\"\"\"\n",
    "        xs = torch.Tensor( np.linspace(0,1,6) )\n",
    "        ys = torch.Tensor( np.random.uniform( size=6 ) )\n",
    "        x_new = torch.Tensor( np.linspace(-.1,1.1,100) )\n",
    "        y_new = Calibrator().interpolate(xs, ys, x_new)    \n",
    "        x_new = x_new.detach().numpy()\n",
    "        y_new = y_new.detach().numpy()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot( x_new, y_new )\n",
    "        ax.scatter( xs.detach().numpy(), ys.detach().numpy() )\n",
    "        plt.show()\n",
    "    def forward(self, x):\n",
    "        return self.interpolate( self.xs, self.ys_actual(), x )    \n",
    "    \n",
    "def inverse_softplus(y):\n",
    "    x = np.log( np.exp(y) - 1 )\n",
    "    return np.where( y > 20, y, x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive weights + parametrized activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monotonic2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Monotonic2,self).__init__()\n",
    "        self.fc1 = LinearPositive(k,16)\n",
    "        self.fc2 = LinearPositive(16,4)\n",
    "        self.fc3 = LinearPositive(4,1)\n",
    "        self.f1 = Calibrator()\n",
    "        self.f2 = Calibrator()\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = self.f1(y)\n",
    "        y = self.fc2(y); y = self.f2(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = torch.nn.Softplus()(y) + 1e-6  # Was: p=y.exp()\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 1h30min for 10,000 epochs; convergence issues\" )\n",
    "\n",
    "x, y, universe = get_data_3( date=DATE1, signs=signs, flip_signs=True )\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Monotonic2(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    \n",
    "    w = model( (x[i,:,:][:,j,:], universe[i,:,:][:,j,:]) )\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR\n",
    "    IRs[t] = IR.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model8_monotonic2_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True, signs=signs, flip_signs=True)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model( (x,universe) ).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model8_monotonic2_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the activation functions learned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
