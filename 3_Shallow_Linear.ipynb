{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from parameters import *\n",
    "from functions2 import *\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lasso_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (data-frame)\" )\n",
    "filename = \"raw/data_ml.csv\"\n",
    "LOG( f\"  Reading {filename} [20 seconds]\" )\n",
    "d = pd.read_csv(filename)\n",
    "d['date'] = pd.to_datetime( d['date'] )\n",
    "\n",
    "predictors = list( signs.keys() )\n",
    "target = 'R1M_Usd'\n",
    "\n",
    "LOG( \"Data (list of matrices)\" )\n",
    "LOG( \"  Reading data/data_ml.pickle\" )\n",
    "dd = load( \"data/data_ml.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning: linear model\n",
    "This is equivalent to the models above, but implemented with a neural network.\n",
    "* Linear model\n",
    "* Change the loss function\n",
    "* Add mini-batches\n",
    "* Add sign constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; forecast the returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data\" )\n",
    "i = np.array([ str(u) < DATE1 for u in d['date'] ]) \n",
    "train = d[i].copy()\n",
    "x = train[ predictors ]\n",
    "y = np.log1p(train[ target ])\n",
    "\n",
    "LOG( \"Clean the data\" )\n",
    "i = np.isfinite(y)\n",
    "x = x[i]\n",
    "y = y[i]\n",
    "\n",
    "x = x.fillna(.5)\n",
    "\n",
    "LOG( \"Model\" )\n",
    "\n",
    "class Linear1(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear1,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "x = torch.tensor(x.values,               dtype=torch.float32)\n",
    "y = torch.tensor(y.values.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "model = Linear1(x.shape[1])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [2 minutes]\" )\n",
    "N = 5000\n",
    "for t in tqdm(range(N)):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Liner model (via pytorch)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model1_linear_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['performance'  ]['period'] = 'all'\n",
    "res['in-sample'    ]['period'] = 'in-sample'\n",
    "res['out-of-sample']['period'] = 'out-of-sample'    \n",
    "r = pd.concat( [ res['performance'], res['in-sample'], res['out-of-sample'] ] )\n",
    "r['model'] = 'linear (1)'\n",
    "r['epochs'] = N\n",
    "r.to_csv(\"results/model1_linear.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model; data as id×signal×date array; forecast the returns\n",
    "\n",
    "Same model as above, but the data is no longer in an (id,date)×signal (2-dimensional) table, but in an id×date×signal 3-dimensional array.\n",
    "\n",
    "The model and the performance are similar but, for some reason, fitting the model is much more time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Training data (3-dimensional array)\" )\n",
    "x, y, universe = get_data_3(date=DATE1, signs=signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Linear2,self).__init__()\n",
    "        self.linear = torch.nn.Linear(k,1)\n",
    "    def forward(self,x):\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.linear(x)   # n×l×1\n",
    "        y = y[:,:,0]         #  n×l\n",
    "        return y\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear2(x.shape[2])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [LONG: 50 minutes for 5000 epochs]\" )\n",
    "N = 5000\n",
    "losses = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    y_pred = model(x) * universe[:,:,0]\n",
    "    loss = criterion(y_pred,y[:,:,0])\n",
    "    losses[t] = loss.item()\n",
    "    pbar.set_description( f\"Loss={loss.item():.5f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( losses )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model2_linear_3d_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "pd.Series(losses).to_csv(\"results/model2_linear_3d_loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the resulting strategy (exactly the same code as above)\n",
    "\n",
    "beta = list( model.parameters() )[0]\n",
    "beta = beta.detach().numpy().flatten()\n",
    "\n",
    "r = None\n",
    "\n",
    "LOG( \"Data for the backtest\" )\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "signal = np.zeros( shape = dd[ list(dd.keys())[0] ].shape )\n",
    "for i,predictor in enumerate(predictors):\n",
    "    signal += beta[i] * dd[predictor].fillna(.5)\n",
    "signal = np.where( dd['universe'], 1, np.nan ) * signal\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.set_yscale('log')\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_title('Linear model (via pytorch)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model2_linear_3d_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['performance'  ]['period'] = 'all'\n",
    "res['in-sample'    ]['period'] = 'in-sample'\n",
    "res['out-of-sample']['period'] = 'out-of-sample'    \n",
    "r = pd.concat( [ res['performance'], res['in-sample'], res['out-of-sample'] ] )\n",
    "r['model'] = 'linear (2)'\n",
    "r['epochs'] = N\n",
    "r.to_csv(\"results/model2_linear_3d.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple, nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not really work: the optimization often remains stuck. \n",
    "Restarting the optimization several (10?) times eventually gives something acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear2(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(NonLinear2,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,x):\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y); # n×l×1\n",
    "        y = y[:,:,0]         #  n×l\n",
    "        return y\n",
    "\n",
    "set_seed(1)\n",
    "    \n",
    "x, y, universe = get_data_3(date=DATE1, signs=signs)\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = NonLinear2(x.shape[2])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "LOG( \"Loop [LONG: 20 minutes for 5000 epochs]\" )\n",
    "N = 100    # Early stopping is needed, and multiple restarts. Mini-batches may fix those convergence and overfitting problems\n",
    "losses = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    y_pred = model(x) * universe[:,:,0]\n",
    "    loss = criterion(y_pred,y[:,:,0])\n",
    "    losses[t] = loss.item()\n",
    "    pbar.set_description( f\"Loss={loss.item():.5f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( losses )\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model2_nonlinear_3d_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "pd.Series(losses).to_csv(\"results/model2_nonlinear_3d_loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, universe = get_data_3(all=True, signs=signs)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal = model(x).detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Non-linear')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model2_nonlinear_3d_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['performance'  ]['period'] = 'all'\n",
    "res['in-sample'    ]['period'] = 'in-sample'\n",
    "res['out-of-sample']['period'] = 'out-of-sample'    \n",
    "r = pd.concat( [ res['performance'], res['in-sample'], res['out-of-sample'] ] )\n",
    "r['model'] = 'nonlinear'\n",
    "r['epochs'] = N\n",
    "r.to_csv(\"results/model2_nonlinear_3d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Done.\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
