{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from functions2 import *\n",
    "from parameters import *\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import lasso_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"Data (data-frame)\" )\n",
    "filename = \"raw/data_ml.csv\"\n",
    "LOG( f\"  Reading {filename} [20 seconds]\" )\n",
    "d = pd.read_csv(filename)\n",
    "d['date'] = pd.to_datetime( d['date'] )\n",
    "\n",
    "predictors = list( signs.keys() )\n",
    "target = 'R1M_Usd'\n",
    "\n",
    "LOG( \"Data (list of matrices)\" )\n",
    "LOG( \"  Reading data/data_ml.pickle\" )\n",
    "dd = load( \"data/data_ml.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty for non-monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This does not work. Try to first estimate a nonlinear model without the penalty (that should work), and only then add the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just the nonlinear model above\n",
    "## Differences: \n",
    "## - Also return the unnormalized score (we want the derivative, and the weights sum to 1)\n",
    "## - requires_grad=True on x\n",
    "## - Penalty term added to the loss function\n",
    "\n",
    "class Monotonic3(torch.nn.Module):\n",
    "    def __init__(self,k):\n",
    "        super(Monotonic3,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(k,16)\n",
    "        self.fc2 = torch.nn.Linear(16,4)\n",
    "        self.fc3 = torch.nn.Linear(4,1)\n",
    "    def forward(self,xs):\n",
    "        x, universe = xs\n",
    "        # x is n×l×k; the linear layer is applied on the last dimension\n",
    "        y = self.fc1(x); y = F.relu(y)\n",
    "        y = self.fc2(y); y = F.relu(y)\n",
    "        y = self.fc3(y)      # n×l×1\n",
    "        p = y.exp()          # Use a softplus instead of an exponential?\n",
    "        p = p * universe\n",
    "        p = p[:,:,0]         #  n×l\n",
    "        p = p / ( 1e-16 + p.sum(axis=0) )  # portolio weights: positive, sum up to 1 for each date\n",
    "        return p, y * universe    # Also return the unnormalized score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG( \"[LONG] 25 minutes for 10,000 epochs\" )\n",
    "\n",
    "x, y, universe = get_data_3(date=DATE1, signs=signs)\n",
    "\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32, requires_grad = True)   ## For the penalty, we will need the gradient wrt x\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "model = Monotonic3(x.shape[2])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "N = 10_000\n",
    "IRs       = np.nan * np.zeros(N)\n",
    "penalties = np.nan * np.zeros(N)\n",
    "pbar = tqdm(range(N))\n",
    "for t in pbar:\n",
    "    \n",
    "    x.shape  # id×date×feature\n",
    "    ## Take half the stocks at random\n",
    "    i = np.random.choice( x.shape[0], x.shape[0] // 2, replace=False ) \n",
    "    ## Take a 3-year period, at random\n",
    "    j = np.random.choice( x.shape[1] - 36 )\n",
    "    j = np.arange( j, j+36 )\n",
    "    input = x[i,:,:][:,j,:]\n",
    "    \n",
    "    w, yhat = model( (input, universe[i,:,:][:,j,:]) )\n",
    "    \n",
    "    ## Penalty\n",
    "    df = torch.autograd.grad(outputs = yhat.sum(), inputs = input, retain_graph = True, create_graph = True )[0]\n",
    "    penalty = 1e3 * (df.clip(max=0)**2).mean()\n",
    "\n",
    "    ratio_returns = w * y[i,:,:][:,j,:][:,:,0].expm1()     # y already contains the forward returns\n",
    "    ratio_returns = ratio_returns.sum(axis=0)\n",
    "    log_returns = ratio_returns.log1p()\n",
    "    IR = log_returns.mean() / log_returns.std()\n",
    "    loss = -IR + penalty\n",
    "    \n",
    "    IRs[t] = IR.item()\n",
    "    penalties[t] = penalty.item()\n",
    "    pbar.set_description( f\"IR={np.nanmean(IRs):.3f} Penalty={penalties[t]:.6f}\" )\n",
    "    if not np.isfinite( loss.item() ):\n",
    "        LOG( f\"{t} PROBLEM\" )\n",
    "        break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "LOG( \"DONE\" )\n",
    "\n",
    "## Add the final IR, on the whole sample\n",
    "w = model( (x,universe) )\n",
    "ratio_returns = w * y[:,:,0].expm1()     # y already contains the forward returns\n",
    "ratio_returns = ratio_returns.sum(axis=0)\n",
    "log_returns = ratio_returns.log1p()\n",
    "IR = log_returns.mean() / log_returns.std()\n",
    "IR = IR.item()\n",
    "\n",
    "## The performance we recorded is very noisy: it is on different periods...\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( 1+np.arange(len(IRs)), IRs )\n",
    "r = lowess( IRs, 1+np.arange(len(IRs)) )\n",
    "ax.plot( r[:,0], r[:,1], color = 'black', linewidth=5 )\n",
    "ax.scatter( 1+len(IRs), IR, color = 'tab:orange', marker='x', s=200, linewidth=5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"IR\")\n",
    "ax.set_xscale('log')\n",
    "fig.savefig(\"plots/model9_monotonic3_loss.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wealth curves\n",
    "##\n",
    "## This is not the strategy actually learned, but the quintile portfolios from the score.\n",
    "## The strategy learned provided actual weights.\n",
    "## \n",
    "\n",
    "x, y, universe = get_data_3(all=True, signs=signs)\n",
    "universe = universe.reshape( y.shape[0], y.shape[1], 1 )\n",
    "y = y.reshape( y.shape[0], y.shape[1], 1 )\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "universe = torch.tensor(universe, dtype=torch.float32)\n",
    "\n",
    "signal, _ = model( (x,universe) )\n",
    "signal = signal.detach().numpy()\n",
    "\n",
    "trailing_log_returns = LAG( np.log1p( dd[ 'R1M_Usd' ] ) )\n",
    "y = trailing_log_returns.copy()\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "assert signal.shape == y.shape\n",
    "signal = pd.DataFrame( signal, index = y.index, columns = y.columns )\n",
    "\n",
    "res = signal_backtest(signal, y, date=DATE1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(6):\n",
    "    ax.plot( res['dates'], res['prices'].iloc[i,:], color = quintile_colours[i] )\n",
    "ax.axvline( pd.to_datetime(DATE1), color='black', linewidth=1 )\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Maximizing the IR (non-linear, signal)')\n",
    "ax.text(0.02, .97, f\"μ={100*res['out-of-sample'].iloc[5,:]['CAGR']:.1f}%\",                  horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .90, f\"σ={100*res['out-of-sample'].iloc[5,:]['Annualized Volatility']:.1f}%\", horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "ax.text(0.02, .83, f\"IR={res['out-of-sample'].iloc[5,:]['Information Ratio']:.2f}\",         horizontalalignment='left', verticalalignment='top', transform = ax.transAxes)\n",
    "fig.savefig(\"plots/model9_monotonic3_wealth.pdf\")\n",
    "plt.show()\n",
    "\n",
    "res['out-of-sample']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
